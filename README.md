# CS238 Final Project: Reinforcement Learning for Minichess Variants

In our project, we propose to apply reinforcement learning to chess variants, with a particular emphasis on so-called "minichess" rulesets. Chess is an oft-studied and widely popular game that is commonly framed as a sequential decision-making problem. Although the rules of chess are relatively simple, substantial uncertainty is generated by the sheer number of possible moves in any given chess position. At increasing degrees of depth, the number of potential positions to evaluate grows exponentially. As an oft-quoted estimate suggests, there are more possible chess games than there are atoms in the Universe. Contemporary approaches to chess engine development have traditionally combined search algorithms with position evaluation heuristics. More recent engines, such as AlphaZero, have applied self-play reinforcement learning (trained using massive computational resources) to achieve equivalent and occasionally superior results. Minichess variants, such Silverman 4Ã—5, reduce the computational intractability of chess by shrinking the board and reducing the number of pieces. Therefore, focusing on minichess retains the fundamental complexity and deep strategy of chess while ameliorating concerns about computational limitations. In our project, we apply reinforcement learning techniques to produce human-level results in minichess variants.
